# -*- coding: utf-8 -*-
"""milestone2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GOe9pRwcOL3kIuuJRGX5ouvInKwCL9r3
"""

import pandas as pd
import numpy as np
import seaborn as sn
import matplotlib as mpl
import matplotlib.pyplot as plt
import itertools
from sklearn import neighbors
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
import pickle
from sklearn.metrics import confusion_matrix
from sklearn import metrics
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.metrics import recall_score
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import precision_score

def convert_to_numeric(dataset):
  lb_make = LabelEncoder()
  dataset['sex'] = lb_make.fit_transform(dataset['sex'])                  #convert each att to numerical values
  dataset['province'] = lb_make.fit_transform(dataset['province'])
  dataset['country'] = lb_make.fit_transform(dataset['country'])
  dataset['date_confirmation'] = lb_make.fit_transform(dataset['date_confirmation'])
  dataset['outcome'] = lb_make.fit_transform(dataset['outcome'])
  return dataset

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()

def visual(cm):
  labels=[ "deceased","hospitalized","nonhospitalized", "recovered"]
  plot_confusion_matrix(cm, labels,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues)

def create_Ada_model(x,y):
  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)
  adaModel = AdaBoostClassifier(n_estimators=1000,learning_rate = 1, random_state=0)    
  #adaModel = AdaBoostClassifier()
  model = adaModel.fit(X_train,y_train)
  filename = 'ada_classifier.pkl'
  pickle.dump(model, open(filename, 'wb'))                                # save the model to disk
  loaded_model = pickle.load(open(filename, 'rb'))                           # load model from disk
  test_result = loaded_model.score(X_test, y_test)                           # show training and testting set accuracy
  train_result = loaded_model.score(X_train, y_train)
  print("test accuracy",test_result)
  print("train accuracy",train_result)
  y_pred = model.predict(X_test)
  matrix = confusion_matrix(y_test, y_pred)
  recall = recall_score(y_test, y_pred, average='macro')
  print('recall')
  print(recall)
  precision = precision_score(y_test, y_pred, average='macro')
  print('precision')
  print(precision)
  return matrix

def create_XGB_model(x,y):
  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)
  XGBModel = GradientBoostingClassifier(n_estimators=150,learning_rate = 0.1, random_state=0)     #tune lr to 0.1 and iteration to 150
  #XGBModel = GradientBoostingClassifier()
  model = XGBModel.fit(X_train,y_train)
  filename = 'xgb_classifier.pkl'
  pickle.dump(model, open(filename, 'wb'))      # save the model to disk
  loaded_model = pickle.load(open(filename, 'rb'))  #load model from disk
  test_result = loaded_model.score(X_test, y_test)  # show training and testting set accuracy
  train_result = loaded_model.score(X_train, y_train)
  print("test accuracy",test_result)
  print("train accuracy",train_result)
  y_pred = model.predict(X_test)
  matrix = confusion_matrix(y_test, y_pred)
  return matrix

def create_KNN_model(x,y):
  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)
  # clf = neighbors.KNeighborsClassifier(n_neighbors=20,leaf_size=20, p=2)                                 #tune n_neighbors, default=5
  #clf = neighbors.KNeighborsClassifier()
  clf = neighbors.KNeighborsClassifier(n_neighbors=12,leaf_size=27,p=1)
  model = clf.fit(X_train, y_train)
  filename = 'KNN_classifier.pkl'
  pickle.dump(model, open(filename, 'wb'))      # save the model to disk
  loaded_model = pickle.load(open(filename, 'rb'))  #load model from disk
  test_result = loaded_model.score(X_test, y_test)  # show training and testting set accuracy
  train_result = loaded_model.score(X_train, y_train)
  print("test accuracy",test_result)
  print("train accuracy",train_result)
  y_pred = model.predict(X_test)
  matrix = confusion_matrix(y_test, y_pred)
  y_true, y_pred = y_test, model.predict(X_test)
  print(classification_report(y_true, y_pred))
  return matrix

def create_att(dataset):
  label = dataset.iloc[:,5]                   # create different labes accroding to "outcome"
  dataset = dataset.drop(['outcome'], axis=1)         
  dataset['label'] = label                    # append label
  x = dataset.iloc[:,0:11]                    # X:df without label
  y = dataset.iloc[:,11]
  return x, y

data = pd.read_csv('result.csv')
data = data.drop(['latitude', 'longitude'], axis=1)
data_numeric = convert_to_numeric(data)
x, y  = create_att(data_numeric)


########### RandomizedSearchCV for knn
k_range=list(range(1,61))
k_range

distributions = dict(n_neighbors = k_range,leaf_size = k_range,p = [1,2,4,5])
knn = neighbors.KNeighborsClassifier(n_neighbors=10,leaf_size=30,p=2)
# knn=neighbors.KNeighborsClassifier()
rand = RandomizedSearchCV(knn,distributions , cv=4, n_iter=10, random_state=5)
rand.fit(x, y)
print(rand.cv_results_)

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)
y_true, y_pred = y_test, rand.predict(X_test)
print(classification_report(y_true, y_pred))

print(rand.best_params_)



############# GridSearchcv for adaboost
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)
adaModel = AdaBoostClassifier()
model = adaModel.fit(X_train,y_train)
param = {'n_estimators': [10,1000],'learning_rate': [0.01, 1]}
clf = GridSearchCV(adaModel,param_grid=param, cv=5,n_jobs = -1)
clf.fit(X_train,y_train)
df = pd.DataFrame(clf.cv_results_)
df.to_csv('adaBoost_tune_result.csv',index=False)
y_pred = clf.predict(X_test)
matrix = confusion_matrix(y_test, y_pred)
print('confusion matrix')
print(matrix)
print('report')
print(classification_report(y_test, y_pred))
print('best parameter')
print(clf.best_params_)

############# GridSearchcv for XGboost
clf = GridSearchCV(GradientBoostingClassifier(),{'n_estimators':[100,150],'max_depth':[3,6,9]},cv=5,return_train_score = False)    #Tunning XGB
clf.fit(x,y)                                                              
print(clf.cv_results_)

from sklearn.metrics import classification_report
X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=0)
y_true,y_pred = y_test, clf.predict(X_test)
print(classification_report(y_true,y_pred))